---
title: "Attrition"
author: "Steven Garrity"
date: "11/16/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Executive Summary:

# Introduction to the Problem
This is a classification problem.

# Load packages
```{r}
library(tidyverse)
library(ggplot2)
library(GGally)
library(mlbench)
library(caret)
library(corrplot)
library(randomForest)
library(knncat)
# library(e1071)
library(fastDummies)
library(doSNOW)
library(parallel)
```

# Set up system for using parallel processing during model training
```{r}
numberofcores = detectCores() # number of cores available on machine
cl <- makeCluster(numberofcores, type = "SOCK")
# Register cluster so that caret will know to train in parallel.
registerDoSNOW(cl)
```

# Read data and plot
```{r}
df <- read.csv('CaseStudy2-data.csv', header=TRUE)
str(df)

# Remove unnecessary columns:
drop <- c("StandardHours","Over18","EmployeeCount","EmployeeNumber","ID")
df <- df[,!(names(df) %in% drop)]
```

# Feature Engineering
```{r}
# Proportion of Total Career Spent at Current Company
df$TotalWorkingYears[df$TotalWorkingYears==0]=0.00001
df$YearsAtCompany[df$YearsAtCompany==0]=0.00001
df <- df %>% mutate(PropYearsCompany = YearsAtCompany/TotalWorkingYears)

# Average Number of Years Per Company
df$NumCompaniesWorked[df$NumCompaniesWorked==0]=0.00001
df <- df %>% mutate(AvgYearsPerCompany = TotalWorkingYears/NumCompaniesWorked)

# Average Years Per Company - Years At Company
df <- df %>% mutate(YrPerCompMinusYrAtCompany = AvgYearsPerCompany - YearsAtCompany)


# Look at the data and see what you see. Like the bear, who went over the mountain. (?)
# helpful guide: https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/


```


```{r}
must_convert <- sapply(df,is.factor)
converted_features <- sapply(df[,must_convert],unclass)
numdf <- cbind(df[,!must_convert],converted_features)
numdf$Attrition <- df$Attrition


# Look for redundant features:
# correlationMatrix <- cor(numdf[,2:dim(numdf)[2]]) # everything but "Attrition"
correlationMatrix <- cor(numdf[,!names(numdf) %in% "Attrition"]) # everything but "Attrition"
print(correlationMatrix)
corrplot(correlationMatrix, method="circle", type="upper", order="hclust")
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75) # recommended cutoff = 0.75
print(highlyCorrelated)
toremove <- colnames(numdf)[highlyCorrelated] # these variables have correlation > 0.5
print(toremove)

# remove the highly correlated features:
numdf$TotalWorkingYears <- NULL
numdf$YearsAtCompany <- NULL
# numdf$MonthlyIncome <- NULL
numdf$PerformanceRating <- NULL
numdf$Department <- NULL

# replot correlation matrix
correlationMatrix <- cor(numdf[,!names(numdf) %in% "Attrition"]) # everything but "Attrition"
print(correlationMatrix)
corrplot(correlationMatrix, method="circle", type="upper", order="hclust")
```

# Rank Features by importance
```{r}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Attrition~., data=numdf, method="lvq", preProcess=c("scale"), search="grid", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=TRUE)
# summarize importance
print(importance)
# plot importance
plot(importance)

CM_lvq <- confusionMatrix(predict(model,numdf[,!names(numdf) %in% "Attrition"]),
                          numdf$Attrition)
CM_lvq
```

# Automated Feature Selection with Recursive Feature Elimination
```{r}
# run the RFE algorithm
#### REMOVE HIGHLY CORRELATED FEATURES BEFORE DOING THIS STEP! ####
x <- numdf 
normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
subsets <- c(5, 10, 12, 14, 16, 18:20)
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 3,
                   verbose = TRUE,
                   allowParallel = TRUE)

results <- rfe(x[,!names(numdf) %in% "Attrition"], x$Attrition, sizes=subsets, rfeControl=ctrl, metric = "Kappa")

# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o")) 

importance <- varImp(results, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
# run the model once with all features. Run pairwise plots of selected features. Check for correlated variables. Next, run the model having removed highly correlated variables that were identified in the lvq varImp steps above. Again, run pairwise plots of selected features and check for correlation. At some point, we should also consider PCA. Once all of this wraps up, proceed with a k-nn and naive bayes classifer. K-NN decision boundaries could make it relatively easy to explain the relationship between the explanatory and response (i.e., Attrition) variables.
```

This will be helpful for later (a much cleaner way of partitioning train/test sets):
library(caret)
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining,]
testing  <- Sonar[-inTraining,]

Helpful resources:
http://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning
http://topepo.github.io/caret/recursive-feature-elimination.html#rfe
https://machinelearningmastery.com/an-introduction-to-feature-selection/
https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

# Try knncat:
```{r}
# first scale numeric observations:
# num_df <- df %>% select("Age","DailyRate","DistanceFromHome","Education","EnvironmentSatisfaction",
#                "HourlyRate","JobInvolvement","JobLevel","JobSatisfaction","MonthlyIncome",
#                "MonthlyRate","NumCompaniesWorked","PercentSalaryHike", "PerformanceRating",
#                "RelationshipSatisfaction","StockOptionLevel",
#                "TotalWorkingYears","TrainingTimesLastYear","WorkLifeBalance",
#                "YearsAtCompany","YearsInCurrentRole","YearsSinceLastPromotion",
#                "YearsWithCurrManager", "PropYearsCompany", "AvgYearsPerCompany",
#                "YrPerCompMinusYrAtCompany")

num_df <- df %>% select("DistanceFromHome","EnvironmentSatisfaction",
               "JobInvolvement","JobLevel","JobSatisfaction","MonthlyIncome",
               "NumCompaniesWorked","StockOptionLevel","TotalWorkingYears","WorkLifeBalance",
               "YearsAtCompany","YearsInCurrentRole","YearsSinceLastPromotion",
               "YearsWithCurrManager", "PropYearsCompany", "AvgYearsPerCompany",
               "YrPerCompMinusYrAtCompany")

scaled_df <- num_df %>% mutate_each(funs(scale(.) %>% as.vector))

cat_df <- df %>% select("BusinessTravel","Department","Gender",
                        "JobRole","MaritalStatus","OverTime")

temp <- cbind(scaled_df,cat_df)
temp$Attrition <- df$Attrition


set.seed(10)
inTraining <- createDataPartition(temp$Attrition, p=0.70, list=FALSE)
train_df <- temp[inTraining,]
test_df <- temp[-inTraining,]

test_knn <- knncat(train_df, test_df, k=c(9,11,13, 15, 18, 21,22,23,25), knots=4, xvals=5, prior.ind=1, verbose=1, classcol = 24)

test_df$binaryAttrition = 0
test_df$binaryAttrition[test_df$Attrition=="Yes"] = 1
test_df$binaryAttrition = as.factor(test_df$binaryAttrition)
test_df$classification <- as.factor(test_knn$test.classes)
confusionMatrix(test_df$classification, test_df$binaryAttrition)

temp1 <- predict(test_knn, train_df, temp[,1:23])
confusionMatrix(table(temp1, temp$Attrition))

### naive bayes classifier
set.seed(10)
inTraining <- createDataPartition(temp$Attrition, p=0.85, list=FALSE)
train_df <- temp[inTraining,]
test_df <- temp[-inTraining,]

model = naiveBayes(Attrition~.,data = train_df, laplace = 100)
table(predict(model,test_df[,c(1:23)]), test_df$Attrition)
CM = confusionMatrix(table(predict(model,test_df[,c(1:23)]), test_df$Attrition))
CM

### naive bayes classifier (without scaling)
set.seed(sample(1:1000,1))
inTraining <- createDataPartition(df$Attrition, p=0.8, list=FALSE)
train_df <- df[inTraining,]
test_df <- df[-inTraining,]
newdata = data.frame(test_df)
newdata$Attrition <- NULL

model = naiveBayes(Attrition~.,data = train_df, laplace = 100)
table(predict(model,newdata), test_df$Attrition)
CM = confusionMatrix(table(predict(model,newdata), test_df$Attrition))
CM

### naive bayes (caret package, without scaling, LOOCV)
# define training control
train_control <- trainControl(method="LOOCV")
# train the model
model <- train(Attrition~., data=df, trControl=train_control, search="grid", method="nb")
# summarize results
print(model)

CM = confusionMatrix(table(predict(model,df), df$Attrition))
CM
```

# KNN
```{r}
num_df <- df %>% select("Attrition","OverTime","AvgYearsPerCompany",
                        "StockOptionLevel","Age","MaritalStatus",
                        "JobInvolvement","JobRole","YearsWithCurrManager",
                        "JobLevel","YearsInCurrentRole","WorkLifeBalance",
                        "JobSatisfaction","MonthlyIncome")

knn_df <- dummy_cols(num_df, select_columns = c("OverTime","MaritalStatus",
                                                "JobRole"), 
                     remove_selected_columns = TRUE)

# set.seed(123)
# seeds <- vector(mode = "list", length = 101)
# for(i in 1:100) seeds[[i]] <- sample.int(1000, 22)
# 
# ## For the last model:
# seeds[[101]] <- sample.int(1000, 1)

ctrl <- trainControl(method = "LOOCV",
                     # repeats = 10,
                     # seeds = seeds,
                     allowParallel = TRUE)

set.seed(1)
mod <- train(Attrition ~ ., data = numdf,
             method = "knn",
             # tuneLength = 13,
             tuneGrid = expand.grid(k = c(2,3,4,5,7,9,11,13,15,18,21,25,29)),
             preProcess = c("scale"),
             trControl = ctrl, 
             metric="Kappa")

CM_knn = confusionMatrix(table(predict(mod, numdf[,!names(numdf) %in% "Attrition"]), knn_df$Attrition))
CM_knn

plot(mod, xlab="k nearest neighbors", main="KNN hyperparameter tuning")
```

# Naive Bayes
```{r}

# # define training control
# train_control <- trainControl(method="LOOCV")
# # train the model
# model <- train(Attrition~., data=knn_df, 
#                trControl=train_control, 
#                search="grid", 
#                metric="Kappa",
#                method="nb")
# # summarize results
# print(model)
# 
# CM = confusionMatrix(table(predict(model,knn_df[,!names(knn_df) %in% "Attrition"]), knn_df$Attrition))
# CM

#############
train_control <- trainControl(method="repeatedcv",
                              repeats = 2,
                              allowParallel = TRUE)

search_grid <- expand.grid(
  usekernel = TRUE,
  fL = 0,
  adjust = seq(1, 8, by = 1))

# train model
nb_A <- train(Attrition~., data=knn_df,
                  method = "nb",
                  trControl = train_control,
                  tuneGrid = search_grid,
                  # search = "grid",
                  metric = "Accuracy",
                  preProcess = c("BoxCox"))

# top 5 models
nb_A$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))

# plot parameter tuning
plot(nb_A)

# confusion matrix
confusionMatrix(nb_A)
pred_A <- predict(nb_A, knn_df[,!names(knn_df) %in% "Attrition"])
confusionMatrix(pred_A,knn_df$Attrition)


########################
##### now optimize Kappa
train_control <- trainControl(method="LOOCV",
                              # repeats = 2,
                              allowParallel = TRUE)

search_grid <- expand.grid(
  usekernel = FALSE,
  laplace = 0:1,
  adjust = seq(1, 4, by = 1))

# train model
library(naivebayes)
nb_K <- train(Attrition~., data=knn_df,
                  method = "naive_bayes",
                  trControl = train_control,
                  tuneGrid = search_grid,
                  # search = "grid",
                  metric = "Kappa",
                  preProcess = c("BoxCox","scale","center"))

# top 5 models
nb_K$results %>% 
  top_n(5, wt = Kappa) %>%
  arrange(desc(Kappa))

# plot parameter tuning
plot(nb_K, main="Naive Bayes Hyperparameter Tuning")

# confusion matrix
confusionMatrix(nb_K)
pred_K <- predict(nb_K, knn_df[,!names(knn_df) %in% "Attrition"])
confusionMatrix(pred_K,knn_df$Attrition)



##### Now combine the two models
pred_df <- data.frame("pred_A" = pred_A, "pred_K" = pred_K)
pred_df$pred_E <- NA
pred_df$pred_E[pred_df$pred_A=="No" & pred_df$pred_K=="No"] <- "No"
pred_df$pred_E[pred_df$pred_A=="Yes" & pred_df$pred_K=="Yes"] <- "Yes"
pred_df$pred_E[pred_df$pred_A=="No" & pred_df$pred_K=="Yes"] <- "No"
pred_df$pred_E[pred_df$pred_A=="Yes" & pred_df$pred_K=="No"] <- "No"

confusionMatrix(as.factor(pred_df$pred_E),knn_df$Attrition)

```

# Logistic Regression
```{r}
scaled_df <- numdf %>% select(-Attrition) %>% mutate_each(funs(scale(.) %>% as.vector))
scaled_df$Attrition <- numdf$Attrition


lfit <- glm(relevel(Attrition, ref="No")~., family=binomial(), data=scaled_df)
summary(lfit)

l_df <- num_df %>% select("Attrition","OverTime","AvgYearsPerCompany","MaritalStatus",
                          "YearsInCurrentRole","JobInvolvement","JobRole","JobSatisfaction")
lfit <- glm(relevel(Attrition, ref="No")~., family=binomial(), data=l_df)
summary(lfit)

ltest <- data.frame("Attrition"=df$Attrition)
ltest$probs <- predict(lfit, type="response")
ltest$predict <- if_else(ltest$probs > 0.5, "Yes","No")

confusionMatrix(as.factor(ltest$predict),df$Attrition)

##############
# try something different
numberofcores = detectCores()
cl <- makeCluster(numberofcores, type = "SOCK")
# Register cluster so that caret will know to train in parallel.
registerDoSNOW(cl)

# variable importance plots are stable, try using the those with importance > 50
ada_df <- numdf %>% select(Attrition, AvgYearsPerCompany, OverTime, MonthlyIncome,
                           YearsAtCompany, StockOptionLevel, MaritalStatus, JobLevel,
                           YearsInCurrentRole, Age, JobInvolvement, JobSatisfaction, 
                           JobRole, DistanceFromHome, EnvironmentSatisfaction,
                           WorkLifeBalance, TrainingTimesLastYear, Education,
                           NumCompaniesWorked, BusinessTravel, DailyRate,
                           MonthlyRate, HourlyRate,RelationshipSatisfaction)

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, allowParallel = TRUE, search="grid")
# m <- train(Attrition~.,data=num_df, 
#                              method = "LogitBoost", 
#                              trControl = ctrl,
#                              nIter=5,
#                              metric = "Accuracy", 
#                              preProc = c("BoxCox","center", "scale"))

searchgrid <- expand.grid(iter=seq(250,750, by=50),
                    maxdepth=c(2,3,4),
                    nu = c(0.075, 0.1, 0.125))

set.seed(12)
m <- train(Attrition~.,data=numdf,
           trControl = ctrl,
           method = "ada",
           # tuneLength = 12,
           tuneGrid=searchgrid,
           metric="Kappa",
           preProc = c("BoxCox","center", "scale"))

confusionMatrix(predict(m,numdf[,!names(numdf) %in% "Attrition"]),df$Attrition)
importance <- varImp(m, scale=TRUE)
# summarize importance
# print(importance)
# plot importance
plot(importance)

ggplot(m)

```




```