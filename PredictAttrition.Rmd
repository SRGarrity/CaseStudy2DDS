---
title: "Attrition"
author: "Steven Garrity"
date: "11/16/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Executive Summary:

# Introduction to the Problem
This is a classification problem.

# Read data and plot
```{r}
library(tidyverse)
library(ggplot2)
library(GGally)
library(mlbench)
library(caret)
library(corrplot)
library(randomForest)

df <- read.csv('CaseStudy2-data.csv', header=TRUE)
str(df)

###SET UP A 6 X 6 GRID AND PLOT HISTOGRAMS FOR ALL VARIABLES###

# df %>% select(Age, Education, Gender, WorkLifeBalance, Attrition) %>% ggpairs(aes(col=Attrition))

# pairs(~Age+DailyRate+DistanceFromHome+Education+HourlyRate+TotalWorkingYears, data=df)

# just found that "StandardHours" = 80 for all observations. Removing from further analysis:
df$StandardHours <- NULL


# Look at the data and see what you see. Like the bear, who went over the mountain. (?)
# helpful guide: https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

numdf <- df[,c("Attrition", "Age","DailyRate","DistanceFromHome","Education","EnvironmentSatisfaction",
               "HourlyRate","JobInvolvement","JobLevel","JobSatisfaction","MonthlyIncome",
               "MonthlyRate","NumCompaniesWorked","PercentSalaryHike",
               "RelationshipSatisfaction","StockOptionLevel",
               "TotalWorkingYears","TrainingTimesLastYear","WorkLifeBalance",
               "YearsAtCompany","YearsInCurrentRole","YearsSinceLastPromotion",
               "YearsWithCurrManager")]

# Look for redundant features:
correlationMatrix <- cor(numdf[,2:dim(numdf)[2]]) # everything but "Attrition"
print(correlationMatrix)
corrplot(correlationMatrix, method="circle", type="upper", order="hclust")
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5) # recommended cutoff = 0.75
print(highlyCorrelated)
toremove <- colnames(numdf)[highlyCorrelated] # these variables have correlation > 0.5
print(toremove)

# Take a moment to plot the correlation matrix of the highlyCorrelated variables:
numdf %>% select(StockOptionLevel, WorkLifeBalance, JobInvolvement, YearsAtCompany, YearsSinceLastPromotion, Attrition) %>%
  ggpairs(aes(col=Attrition))

pairs(~StockOptionLevel+WorkLifeBalance+JobInvolvement+YearsAtCompany+
        YearsSinceLastPromotion, data=numdf)

# now remove highly correlated variables and continue...Not clear if this is the correct way to proceed
for (i in 1:length(toremove)) {
  numdf[,toremove[i]] = NULL
}


# Rank Features by importance:
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Attrition~., data=numdf, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=TRUE)
# summarize importance
print(importance)
# plot importance
plot(importance)

# Feature Selection
# define the control using a random forest selection function
# control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# results <- rfe(numdf[,2:dim(numdf)[2]], numdf[,1], sizes=c(1:10), rfeControl=control)
# run the RFE algorithm
x <- numdf #### REMOVE HIGHLY CORRELATED FEATURES BEFORE DOING THIS STEP! ####
normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
subsets <- c(5, 10, 12, 14, 16, 18:20)
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 3,
                   verbose = TRUE)

results <- rfe(x[,2:dim(x)[2]], x[,1], sizes=subsets, rfeControl=ctrl)

# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o")) # run the model once with all features. Run pairwise plots of selected features. Check for correlated variables. Next, run thee model having removed highly correlated variables that were identified in the lvq varImp steps above. Again, run pairwise plots of selected features and check for correlation. At some point, we should also consider PCA. Once all of this wraps up, proceed with a k-nn and naive bayes classifer. K-NN decision boundaries could make it relatively easy to explain the relationship between the explanatory and response (i.e., Attrition) variables.

#### A Stepwise Approach ####...Not going to work here, because this is a classification problem, not a regression problem.
library(MASS)
stepmodel <- lm(Attrition~., data = numdf)
step <- stepAIC(stepmodel, direction = "both", trace = TRUE)
step

step.model <- train(Attrition ~., data = numdf,
                    method = "lmStepAIC", 
                    trControl = train.control,
                    trace = FALSE
                    )
# Model accuracy
step.model$results
```

This will be helpful for later (a much cleaner way of partitioning train/test sets):
library(caret)
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining,]
testing  <- Sonar[-inTraining,]

Helpful resources:
http://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning
http://topepo.github.io/caret/recursive-feature-elimination.html#rfe
https://machinelearningmastery.com/an-introduction-to-feature-selection/
https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/
